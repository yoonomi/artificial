"""
é«˜çº§æ¨ç†æ™ºèƒ½ä½“æµ‹è¯•è„šæœ¬

ä¸“é—¨æµ‹è¯•é«˜çº§æ¨ç†æ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼ŒéªŒè¯å…¶æ˜¯å¦ä¸¥æ ¼æŒ‰ç…§"å‘ç°â†’éªŒè¯â†’å†™å…¥"çš„é¡ºåºæ‰§è¡Œã€‚
"""

import autogen
import logging
from typing import Dict, Any

from config import config
from tools.graph_db import GraphDB
from tools.reasoning_tools import (
    find_interesting_patterns,
    verify_hypothesis_from_text,
    create_inferred_relationship
)
from agents.advanced_reasoning_agent import (
    create_advanced_reasoning_agent,
    create_reasoning_user_proxy
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def register_reasoning_functions(agent: autogen.AssistantAgent, user_proxy: autogen.UserProxyAgent, graph_db: GraphDB):
    """ä¸ºæ™ºèƒ½ä½“æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°"""
    
    def find_patterns_wrapper():
        """å‘ç°å›¾è°±ä¸­çš„æœ‰è¶£æ¨¡å¼"""
        logger.info("ğŸ” æ‰§è¡Œæ¨¡å¼å‘ç°...")
        result = find_interesting_patterns(graph_db)
        logger.info(f"ğŸ“Š å‘ç°äº† {len(result)} ä¸ªæ½œåœ¨æ¨¡å¼")
        return result
    
    def verify_hypothesis_wrapper(pattern: Dict[str, Any]):
        """éªŒè¯æ¨¡å¼å‡è®¾"""
        pattern_type = pattern.get('type', 'æœªçŸ¥')
        logger.info(f"ğŸ§ éªŒè¯æ¨¡å¼: {pattern_type}")
        result = verify_hypothesis_from_text(pattern, graph_db)
        confidence = result.get('verification', 'æœªçŸ¥')
        logger.info(f"âœ… éªŒè¯ç»“æœ: {confidence}")
        return result
    
    def create_relationship_wrapper(verified_pattern: Dict[str, Any]):
        """åˆ›å»ºæ¨ç†å…³ç³»"""
        pattern_type = verified_pattern.get('type', 'æœªçŸ¥')
        confidence = verified_pattern.get('verification', 'æœªçŸ¥')
        logger.info(f"ğŸ”— åˆ›å»ºæ¨ç†å…³ç³»: {pattern_type} (ç½®ä¿¡åº¦: {confidence})")
        result = create_inferred_relationship(verified_pattern, graph_db)
        logger.info(f"ğŸ’¾ åˆ›å»ºç»“æœ: {result}")
        return result
    
    # æ³¨å†Œå‡½æ•°åˆ°æ™ºèƒ½ä½“
    autogen.register_function(
        find_patterns_wrapper,
        caller=agent,
        executor=user_proxy,
        name="find_interesting_patterns",
        description="åœ¨çŸ¥è¯†å›¾è°±ä¸­å‘ç°æ½œåœ¨çš„æœ‰è¶£æ¨¡å¼ï¼Œç”¨äºæ¨ç†åˆ†æ"
    )
    
    autogen.register_function(
        verify_hypothesis_wrapper,
        caller=agent,
        executor=user_proxy,
        name="verify_hypothesis_from_text",
        description="éªŒè¯å‘ç°çš„æ¨¡å¼æ˜¯å¦æ„æˆæœ‰æ„ä¹‰çš„éšå«å…³ç³»"
    )
    
    autogen.register_function(
        create_relationship_wrapper,
        caller=agent,
        executor=user_proxy,
        name="create_inferred_relationship",
        description="å°†éªŒè¯ä¸ºé«˜ç½®ä¿¡åº¦çš„éšå«å…³ç³»å†™å…¥å›¾è°±"
    )


def setup_test_data(db: GraphDB):
    """è®¾ç½®æµ‹è¯•æ•°æ®"""
    logger.info("ğŸ› ï¸  è®¾ç½®æµ‹è¯•æ•°æ®...")
    
    # æ¸…ç†æ—§æ•°æ®
    cleanup_query = """
    MATCH (n) WHERE n.name IN [
        'æµ‹è¯•å¼ æ•™æˆ', 'æµ‹è¯•æåšå£«', 'æµ‹è¯•ç‹ç ”ç©¶å‘˜', 'æµ‹è¯•é™ˆå­¦ç”Ÿ',
        'æµ‹è¯•å¤§å­¦', 'æµ‹è¯•ç ”ç©¶æ‰€', 'æµ‹è¯•AIé¡¹ç›®', 'æµ‹è¯•æ·±åº¦å­¦ä¹ è®ºæ–‡'
    ] DETACH DELETE n
    """
    db.execute_query(cleanup_query)
    
    # åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹å’Œå…³ç³»
    test_data = [
        # åˆ›å»ºäººç‰©
        ("CREATE (p:äººç‰© {name: 'æµ‹è¯•å¼ æ•™æˆ', position: 'æ•™æˆ', age: 45})", {}),
        ("CREATE (p:äººç‰© {name: 'æµ‹è¯•æåšå£«', position: 'åšå£«å', age: 32})", {}),
        ("CREATE (p:äººç‰© {name: 'æµ‹è¯•ç‹ç ”ç©¶å‘˜', position: 'ç ”ç©¶å‘˜', age: 38})", {}),
        ("CREATE (p:äººç‰© {name: 'æµ‹è¯•é™ˆå­¦ç”Ÿ', position: 'åšå£«ç”Ÿ', age: 26})", {}),
        
        # åˆ›å»ºæœºæ„
        ("CREATE (o:æœºæ„ {name: 'æµ‹è¯•å¤§å­¦', type: 'å¤§å­¦'})", {}),
        ("CREATE (o:æœºæ„ {name: 'æµ‹è¯•ç ”ç©¶æ‰€', type: 'ç ”ç©¶æ‰€'})", {}),
        
        # åˆ›å»ºé¡¹ç›®å’Œè®ºæ–‡
        ("CREATE (proj:é¡¹ç›® {name: 'æµ‹è¯•AIé¡¹ç›®', budget: 1000000})", {}),
        ("CREATE (paper:è®ºæ–‡ {name: 'æµ‹è¯•æ·±åº¦å­¦ä¹ è®ºæ–‡', year: 2023})", {}),
        
        # åˆ›å»ºå·¥ä½œå…³ç³»
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•å¼ æ•™æˆ'}), (o:æœºæ„ {name: 'æµ‹è¯•å¤§å­¦'})
        CREATE (p)-[:å·¥ä½œäº {
            source_sentence: 'å¼ æ•™æˆè‡ª2010å¹´èµ·åœ¨æµ‹è¯•å¤§å­¦è®¡ç®—æœºå­¦é™¢æ‹…ä»»æ•™æˆï¼Œä¸»è¦ç ”ç©¶äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ã€‚'
        }]->(o)
        """, {}),
        
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•æåšå£«'}), (o:æœºæ„ {name: 'æµ‹è¯•å¤§å­¦'})
        CREATE (p)-[:å·¥ä½œäº {
            source_sentence: 'æåšå£«åœ¨æµ‹è¯•å¤§å­¦è¿›è¡Œåšå£«åç ”ç©¶ï¼Œä¸“æ³¨äºæ·±åº¦å­¦ä¹ ç®—æ³•ä¼˜åŒ–ã€‚'
        }]->(o)
        """, {}),
        
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•ç‹ç ”ç©¶å‘˜'}), (o:æœºæ„ {name: 'æµ‹è¯•ç ”ç©¶æ‰€'})
        CREATE (p)-[:å·¥ä½œäº {
            source_sentence: 'ç‹ç ”ç©¶å‘˜åœ¨æµ‹è¯•ç ”ç©¶æ‰€ä»äº‹äººå·¥æ™ºèƒ½åŸºç¡€ç†è®ºç ”ç©¶å·¥ä½œã€‚'
        }]->(o)
        """, {}),
        
        # åˆ›å»ºå­¦ä¹ å…³ç³»
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•é™ˆå­¦ç”Ÿ'}), (o:æœºæ„ {name: 'æµ‹è¯•å¤§å­¦'})
        CREATE (p)-[:å°±è¯»äº {
            source_sentence: 'é™ˆå­¦ç”Ÿåœ¨æµ‹è¯•å¤§å­¦æ”»è¯»è®¡ç®—æœºç§‘å­¦åšå£«å­¦ä½ï¼Œå¸ˆä»å¼ æ•™æˆã€‚'
        }]->(o)
        """, {}),
        
        # åˆ›å»ºé¡¹ç›®å‚ä¸å…³ç³»
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•å¼ æ•™æˆ'}), (proj:é¡¹ç›® {name: 'æµ‹è¯•AIé¡¹ç›®'})
        CREATE (p)-[:å‚ä¸ {
            source_sentence: 'å¼ æ•™æˆä½œä¸ºé¡¹ç›®è´Ÿè´£äººï¼Œé¢†å¯¼æµ‹è¯•AIé¡¹ç›®çš„ç ”ç©¶å·¥ä½œã€‚'
        }]->(proj)
        """, {}),
        
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•æåšå£«'}), (proj:é¡¹ç›® {name: 'æµ‹è¯•AIé¡¹ç›®'})
        CREATE (p)-[:å‚ä¸ {
            source_sentence: 'æåšå£«åœ¨æµ‹è¯•AIé¡¹ç›®ä¸­è´Ÿè´£æ ¸å¿ƒç®—æ³•çš„è®¾è®¡å’Œå®ç°ã€‚'
        }]->(proj)
        """, {}),
        
        # åˆ›å»ºè®ºæ–‡å‘è¡¨å…³ç³»
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•å¼ æ•™æˆ'}), (paper:è®ºæ–‡ {name: 'æµ‹è¯•æ·±åº¦å­¦ä¹ è®ºæ–‡'})
        CREATE (p)-[:å‘è¡¨ {
            source_sentence: 'å¼ æ•™æˆä¸å›¢é˜Ÿæˆå‘˜åˆä½œå‘è¡¨äº†å…³äºæ·±åº¦å­¦ä¹ çš„é‡è¦è®ºæ–‡ã€‚'
        }]->(paper)
        """, {}),
        
        ("""
        MATCH (p:äººç‰© {name: 'æµ‹è¯•é™ˆå­¦ç”Ÿ'}), (paper:è®ºæ–‡ {name: 'æµ‹è¯•æ·±åº¦å­¦ä¹ è®ºæ–‡'})
        CREATE (p)-[:å‚ä¸ {
            source_sentence: 'é™ˆå­¦ç”Ÿä½œä¸ºç¬¬äºŒä½œè€…å‚ä¸äº†æ·±åº¦å­¦ä¹ è®ºæ–‡çš„æ’°å†™å·¥ä½œã€‚'
        }]->(paper)
        """, {}),
    ]
    
    # æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
    for query, params in test_data:
        db.execute_query(query, params)
    
    logger.info("âœ… æµ‹è¯•æ•°æ®è®¾ç½®å®Œæˆ")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§  é«˜çº§æ¨ç†æ™ºèƒ½ä½“æµ‹è¯•å¼€å§‹")
    logger.info("=" * 60)
    
    try:
        # 1. è¿æ¥æ•°æ®åº“
        logger.info("ğŸ”— è¿æ¥Neo4jæ•°æ®åº“...")
        db = GraphDB(
            uri=config.NEO4J_URI,
            username=config.NEO4J_USERNAME,
            password=config.NEO4J_PASSWORD
        )
        
        if not db.connected:
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # 2. è®¾ç½®æµ‹è¯•æ•°æ®
        setup_test_data(db)
        
        # 3. åˆ›å»ºæ¨ç†æ™ºèƒ½ä½“
        logger.info("\nğŸ¤– åˆ›å»ºæ¨ç†æ™ºèƒ½ä½“...")
        
        llm_config = config.llm_config_gpt4
        reasoning_agent = create_advanced_reasoning_agent(llm_config)
        reasoning_proxy = create_reasoning_user_proxy()
        
        logger.info("âœ… æ¨ç†æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        
        # 4. æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°
        logger.info("\nğŸ”§ æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°...")
        register_reasoning_functions(reasoning_agent, reasoning_proxy, db)
        logger.info("âœ… å·¥å…·å‡½æ•°æ³¨å†Œå®Œæˆ")
        
        # 5. å¯åŠ¨æ¨ç†ä»»åŠ¡
        logger.info("\nğŸ” å¯åŠ¨æ¨ç†åˆ†æä»»åŠ¡...")
        logger.info("-" * 40)
        
        reasoning_task = "è¯·å¼€å§‹å¯¹ç°æœ‰å›¾è°±è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æï¼Œå‘ç°å¹¶éªŒè¯éšå«çš„çŸ¥è¯†å…³ç³»ã€‚ä¸¥æ ¼æŒ‰ç…§å‘ç°â†’éªŒè¯â†’å†™å…¥çš„å·¥ä½œæµç¨‹æ‰§è¡Œã€‚"
        
        # å¼€å§‹å¯¹è¯
        reasoning_proxy.initiate_chat(
            reasoning_agent,
            message=reasoning_task,
            max_turns=25  # å…è®¸è¶³å¤Ÿçš„è½®æ¬¡
        )
        
        # 6. éªŒè¯ç»“æœ
        logger.info("\nğŸ“Š éªŒè¯æ¨ç†ç»“æœ...")
        logger.info("-" * 40)
        
        # æŸ¥è¯¢æ¨ç†å…³ç³»
        inferred_query = """
        MATCH (a)-[r]->(b) 
        WHERE r.type = 'INFERRED'
        RETURN a.name as source, type(r) as rel_type, b.name as target, 
               r.confidence as confidence, r.pattern_type as pattern_type
        ORDER BY r.confidence DESC
        """
        
        inferred_relations = db.execute_query(inferred_query)
        
        if inferred_relations:
            logger.info(f"ğŸ‰ æ™ºèƒ½ä½“æˆåŠŸåˆ›å»ºäº† {len(inferred_relations)} ä¸ªæ¨ç†å…³ç³»:")
            for i, rel in enumerate(inferred_relations, 1):
                logger.info(f"   {i}. {rel['source']} --[{rel['rel_type']}]--> {rel['target']}")
                logger.info(f"      ç½®ä¿¡åº¦: {rel['confidence']}, æ¨¡å¼: {rel['pattern_type']}")
        else:
            logger.info("âš ï¸  æœªå‘ç°æ–°çš„æ¨ç†å…³ç³»")
        
        # 7. æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
        
        # æ¸…ç†æµ‹è¯•èŠ‚ç‚¹
        cleanup_query = """
        MATCH (n) WHERE n.name CONTAINS 'æµ‹è¯•'
        DETACH DELETE n
        """
        db.execute_query(cleanup_query)
        
        # æ¸…ç†æ¨ç†å…³ç³»
        cleanup_inferred = "MATCH ()-[r]->() WHERE r.type = 'INFERRED' DELETE r"
        db.execute_query(cleanup_inferred)
        
        logger.info("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        
        logger.info("\nğŸ¯ é«˜çº§æ¨ç†æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if 'db' in locals() and hasattr(db, 'close'):
            db.close()


if __name__ == "__main__":
    main() 