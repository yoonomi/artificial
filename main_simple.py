"""
AutoGençŸ¥è¯†å›¾è°±ç”Ÿæˆç³»ç»Ÿ - ç®€åŒ–ç‰ˆæœ¬

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç‰ˆæœ¬ï¼Œä½¿ç”¨æ›´å°‘çš„æ™ºèƒ½ä½“å’Œæ›´ç›´æ¥çš„å·¥ä½œæµï¼Œ
ä¸“é—¨ç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºæ ¸å¿ƒåŠŸèƒ½ã€‚
"""

import autogen
import logging
import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# å¯¼å…¥é…ç½®å’Œå·¥å…·
from config import config
from tools.graph_db import GraphDB

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    stream=sys.stdout
)
logger = logging.getLogger(__name__)


def create_simplified_ontologist_agent(llm_config: Dict) -> autogen.AssistantAgent:
    """åˆ›å»ºç®€åŒ–çš„æœ¬ä½“è®ºä¸“å®¶æ™ºèƒ½ä½“"""
    
    system_message = """ä½ æ˜¯ä¸€ä½æœ¬ä½“è®ºä¸“å®¶ã€‚è¯·åˆ†ææ–‡æœ¬å¹¶è®¾è®¡çŸ¥è¯†å›¾è°±æ¶æ„ã€‚

è¾“å‡ºè¦æ±‚ï¼š
å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
{
  "node_labels": ["å®ä½“ç±»å‹1", "å®ä½“ç±»å‹2", ...],
  "relationship_types": ["å…³ç³»ç±»å‹1", "å…³ç³»ç±»å‹2", ...]
}

ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¿”å›JSONã€‚"""

    return autogen.AssistantAgent(
        name="SimplifiedOntologist",
        system_message=system_message,
        llm_config=llm_config,
        max_consecutive_auto_reply=1,
    )


def create_simplified_extractor_agent(llm_config: Dict) -> autogen.AssistantAgent:
    """åˆ›å»ºç®€åŒ–çš„å®ä½“æŠ½å–æ™ºèƒ½ä½“"""
    
    system_message = """ä½ æ˜¯å®ä½“æŠ½å–ä¸“å®¶ã€‚æ ¹æ®æä¾›çš„æœ¬ä½“æ¶æ„ï¼Œä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ã€‚

è¾“å‡ºè¦æ±‚ï¼š
å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š
{
  "nodes": [
    {
      "id": "N001",
      "label": "èŠ‚ç‚¹åç§°", 
      "type": "èŠ‚ç‚¹ç±»å‹",
      "properties": {"name": "åç§°", "description": "æè¿°"}
    }
  ],
  "edges": [
    {
      "id": "E001",
      "source": "N001",
      "target": "N002", 
      "type": "å…³ç³»ç±»å‹",
      "properties": {"confidence": 0.9}
    }
  ]
}

ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¿”å›JSONã€‚"""

    return autogen.AssistantAgent(
        name="SimplifiedExtractor",
        system_message=system_message,
        llm_config=llm_config,
        max_consecutive_auto_reply=1,
    )


class SimplifiedWorkflow:
    """ç®€åŒ–çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆå·¥ä½œæµ"""
    
    def __init__(self):
        self.config = config
        self.llm_config = self.config.llm_config_gpt4
        self.graph_db = None
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            self.graph_db = GraphDB(
                uri=self.config.NEO4J_URI,
                username=self.config.NEO4J_USERNAME,
                password=self.config.NEO4J_PASSWORD
            )
            if self.graph_db.connected:
                logger.info("âœ… Neo4jæ•°æ®åº“è¿æ¥æˆåŠŸ")
            else:
                logger.warning("âš ï¸ Neo4jæ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå°†è·³è¿‡æ•°æ®åº“ä¿å­˜")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.graph_db = None
    
    def load_sample_text(self, file_path: str = "data/sample_text.txt") -> str:
        """åŠ è½½ç¤ºä¾‹æ–‡æœ¬ï¼ˆæˆªå–å‰500å­—ç¬¦ä»¥å‡å°‘å¤„ç†æ—¶é—´ï¼‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read().strip()
            
            # æˆªå–å‰800å­—ç¬¦ä»¥å‡å°‘å¤„ç†æ—¶é—´å’Œç½‘ç»œè¯·æ±‚å¤§å°
            if len(text) > 800:
                text = text[:800] + "..."
                
            logger.info(f"ğŸ“– æˆåŠŸåŠ è½½æ–‡æœ¬æ–‡ä»¶: {file_path} ({len(text)} å­—ç¬¦)")
            return text
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            return """
äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å†ç¨‹

1950å¹´ï¼Œé˜¿å…°Â·å›¾çµæå‡ºäº†å›¾çµæµ‹è¯•ã€‚
1956å¹´ï¼Œçº¦ç¿°Â·éº¦å¡é”¡åœ¨è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šé¦–æ¬¡æå‡º"äººå·¥æ™ºèƒ½"æ¦‚å¿µã€‚
2016å¹´ï¼Œè°·æ­Œçš„AlphaGoå‡»è´¥äº†å›´æ£‹ä¸–ç•Œå† å†›æä¸–çŸ³ã€‚
2022å¹´ï¼ŒOpenAIå‘å¸ƒäº†ChatGPTï¼Œå¼•å‘äº†æ–°ä¸€è½®AIçƒ­æ½®ã€‚
            """.strip()
    
    def step1_design_ontology(self, text: str) -> Dict:
        """æ­¥éª¤1ï¼šè®¾è®¡æœ¬ä½“æ¶æ„"""
        logger.info("ğŸ” æ­¥éª¤1: è®¾è®¡æœ¬ä½“æ¶æ„...")
        
        try:
            ontologist = create_simplified_ontologist_agent(self.llm_config)
            user_proxy = autogen.UserProxyAgent(
                name="UserProxy",
                human_input_mode="NEVER",
                max_consecutive_auto_reply=0,
                code_execution_config=False,
            )
            
            message = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å¹¶è®¾è®¡æœ¬ä½“æ¶æ„ï¼š\n\n{text}"
            
            # å¯åŠ¨å¯¹è¯
            user_proxy.initiate_chat(ontologist, message=message, max_turns=1)
            
            # è·å–ç»“æœ
            last_message = user_proxy.last_message()["content"]
            logger.info(f"ğŸ“‹ æœ¬ä½“è®¾è®¡ç»“æœ: {last_message}")
            
            # è§£æJSON
            try:
                start_idx = last_message.find('{')
                end_idx = last_message.rfind('}') + 1
                
                if start_idx != -1 and end_idx > start_idx:
                    json_str = last_message[start_idx:end_idx]
                    ontology = json.loads(json_str)
                    logger.info("âœ… æœ¬ä½“æ¶æ„è®¾è®¡æˆåŠŸ")
                    return ontology
                else:
                    logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                    return None
                    
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æœ¬ä½“è®¾è®¡å¤±è´¥: {e}")
            return None
    
    def step2_extract_knowledge(self, text: str, ontology: Dict) -> Dict:
        """æ­¥éª¤2ï¼šæŠ½å–çŸ¥è¯†å›¾è°±"""
        logger.info("ğŸ” æ­¥éª¤2: æŠ½å–çŸ¥è¯†å›¾è°±...")
        
        try:
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(3)
            
            extractor = create_simplified_extractor_agent(self.llm_config)
            user_proxy = autogen.UserProxyAgent(
                name="UserProxy",
                human_input_mode="NEVER",
                max_consecutive_auto_reply=0,
                code_execution_config=False,
            )
            
            message = f"""
æ ¹æ®ä»¥ä¸‹æœ¬ä½“æ¶æ„ä»æ–‡æœ¬ä¸­æŠ½å–çŸ¥è¯†å›¾è°±ï¼š

æœ¬ä½“æ¶æ„ï¼š
{json.dumps(ontology, ensure_ascii=False, indent=2)}

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·æŠ½å–å®ä½“å’Œå…³ç³»æ„å»ºçŸ¥è¯†å›¾è°±ã€‚
"""
            
            # å¯åŠ¨å¯¹è¯
            user_proxy.initiate_chat(extractor, message=message, max_turns=1)
            
            # è·å–ç»“æœ
            last_message = user_proxy.last_message()["content"]
            logger.info(f"ğŸ“‹ çŸ¥è¯†æŠ½å–ç»“æœ: {last_message}")
            
            # è§£æJSON
            try:
                start_idx = last_message.find('{')
                end_idx = last_message.rfind('}') + 1
                
                if start_idx != -1 and end_idx > start_idx:
                    json_str = last_message[start_idx:end_idx]
                    knowledge_graph = json.loads(json_str)
                    logger.info("âœ… çŸ¥è¯†å›¾è°±æŠ½å–æˆåŠŸ")
                    return knowledge_graph
                else:
                    logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                    return None
                    
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†æŠ½å–å¤±è´¥: {e}")
            return None
    
    def step3_save_to_database(self, knowledge_graph: Dict) -> bool:
        """æ­¥éª¤3ï¼šä¿å­˜åˆ°æ•°æ®åº“"""
        logger.info("ğŸ” æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“...")
        
        if not self.graph_db or not self.graph_db.connected:
            logger.warning("âš ï¸ æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡ä¿å­˜")
            return False
        
        try:
            success = self.graph_db.import_knowledge_graph(knowledge_graph)
            if success:
                logger.info("âœ… çŸ¥è¯†å›¾è°±å·²æˆåŠŸä¿å­˜åˆ°Neo4jæ•°æ®åº“")
            else:
                logger.error("âŒ çŸ¥è¯†å›¾è°±ä¿å­˜å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            return False
    
    def run_simplified_workflow(self, input_text: str):
        """è¿è¡Œç®€åŒ–çš„å·¥ä½œæµ"""
        logger.info("ğŸš€ å¼€å§‹ç®€åŒ–çŸ¥è¯†å›¾è°±ç”Ÿæˆå·¥ä½œæµ...")
        
        try:
            # æ­¥éª¤1ï¼šè®¾è®¡æœ¬ä½“æ¶æ„
            ontology = self.step1_design_ontology(input_text)
            if not ontology:
                logger.error("âŒ æœ¬ä½“è®¾è®¡å¤±è´¥ï¼Œå·¥ä½œæµä¸­æ­¢")
                return
            
            # æ­¥éª¤2ï¼šæŠ½å–çŸ¥è¯†å›¾è°±
            knowledge_graph = self.step2_extract_knowledge(input_text, ontology)
            if not knowledge_graph:
                logger.error("âŒ çŸ¥è¯†æŠ½å–å¤±è´¥ï¼Œå·¥ä½œæµä¸­æ­¢")
                return
            
            # æ­¥éª¤3ï¼šä¿å­˜åˆ°æ•°æ®åº“
            self.step3_save_to_database(knowledge_graph)
            
            # è¾“å‡ºç»“æœæ‘˜è¦
            self.print_summary(ontology, knowledge_graph)
            
            logger.info("âœ… ç®€åŒ–å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def print_summary(self, ontology: Dict, knowledge_graph: Dict):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š çŸ¥è¯†å›¾è°±ç”Ÿæˆæ‘˜è¦")
        logger.info("=" * 60)
        
        if ontology:
            logger.info(f"ğŸ—ï¸ å®ä½“ç±»å‹æ•°é‡: {len(ontology.get('node_labels', []))}")
            logger.info(f"ğŸ”— å…³ç³»ç±»å‹æ•°é‡: {len(ontology.get('relationship_types', []))}")
        
        if knowledge_graph:
            logger.info(f"ğŸ“ˆ ç”ŸæˆèŠ‚ç‚¹æ•°: {len(knowledge_graph.get('nodes', []))}")
            logger.info(f"ğŸ”— ç”Ÿæˆè¾¹æ•°: {len(knowledge_graph.get('edges', []))}")
        
        logger.info("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ§  AutoGençŸ¥è¯†å›¾è°±ç”Ÿæˆç³»ç»Ÿå¯åŠ¨ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    try:
        # æ£€æŸ¥APIå¯†é’¥é…ç½®
        if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == "your_openai_api_key_here":
            logger.error("âŒ OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
            return
        
        # åˆ›å»ºç®€åŒ–å·¥ä½œæµç®¡ç†å™¨
        workflow = SimplifiedWorkflow()
        
        # åŠ è½½ç¤ºä¾‹æ–‡æœ¬
        sample_text = workflow.load_sample_text()
        
        logger.info(f"ğŸ“ å¤„ç†æ–‡æœ¬é•¿åº¦: {len(sample_text)} å­—ç¬¦")
        logger.info(f"ğŸ“ æ–‡æœ¬é¢„è§ˆ: {sample_text[:100]}...")
        
        # è¿è¡Œç®€åŒ–å·¥ä½œæµ
        workflow.run_simplified_workflow(sample_text)
        
        logger.info("ğŸ‰ AutoGençŸ¥è¯†å›¾è°±ç”Ÿæˆç³»ç»Ÿæ‰§è¡Œå®Œæˆï¼")
        logger.info("ğŸ’¡ è¯·æ£€æŸ¥Neo4j BrowseræŸ¥çœ‹ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±")
        logger.info("ğŸ” æŸ¥è¯¢å‘½ä»¤: MATCH (n) RETURN n LIMIT 25")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 