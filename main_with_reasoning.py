"""
AutoGençŸ¥è¯†å›¾è°±ç”Ÿæˆç³»ç»Ÿ - å«é«˜çº§æ¨ç†åŠŸèƒ½

è¿™ä¸ªç‰ˆæœ¬åŒ…å«å®Œæ•´çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆæµç¨‹ï¼Œä»¥åŠé«˜çº§æ¨ç†æ™ºèƒ½ä½“çš„é›†æˆã€‚
å·¥ä½œæµç¨‹ï¼šæ–‡æœ¬åˆ†æ -> å®ä½“æå– -> å›¾è°±æ„å»º -> æ·±åº¦æ¨ç†åˆ†æ
"""

import autogen
import logging
import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# å¯¼å…¥é…ç½®å’Œå·¥å…·
from config import config
from tools.graph_db import GraphDB
from tools.reasoning_tools import (
    find_interesting_patterns,
    verify_hypothesis_from_text,
    create_inferred_relationship
)
from agents.advanced_reasoning_agent import (
    create_advanced_reasoning_agent,
    create_reasoning_user_proxy
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    stream=sys.stdout
)
logger = logging.getLogger(__name__)


def create_simplified_ontologist_agent(llm_config: Dict) -> autogen.AssistantAgent:
    """åˆ›å»ºç®€åŒ–çš„æœ¬ä½“è®ºä¸“å®¶æ™ºèƒ½ä½“"""
    
    system_message = """ä½ æ˜¯ä¸€ä½æœ¬ä½“è®ºä¸“å®¶ã€‚è¯·åˆ†ææ–‡æœ¬å¹¶è®¾è®¡çŸ¥è¯†å›¾è°±æ¶æ„ã€‚

è¾“å‡ºè¦æ±‚ï¼š
å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
{
  "node_labels": ["å®ä½“ç±»å‹1", "å®ä½“ç±»å‹2", ...],
  "relationship_types": ["å…³ç³»ç±»å‹1", "å…³ç³»ç±»å‹2", ...]
}

ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¿”å›JSONã€‚"""

    return autogen.AssistantAgent(
        name="SimplifiedOntologist",
        system_message=system_message,
        llm_config=llm_config,
        max_consecutive_auto_reply=1,
    )


def create_entity_extraction_agent(llm_config: Dict) -> autogen.AssistantAgent:
    """åˆ›å»ºå®ä½“æå–æ™ºèƒ½ä½“"""
    
    system_message = """ä½ æ˜¯ä¸€ä½å®ä½“æå–ä¸“å®¶ã€‚è¯·ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚

è¾“å‡ºè¦æ±‚ï¼š
å¿…é¡»è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š
{
  "entities": [
    {"name": "å®ä½“å", "type": "å®ä½“ç±»å‹", "properties": {"å±æ€§": "å€¼"}},
    ...
  ],
  "relationships": [
    {"source": "æºå®ä½“å", "target": "ç›®æ ‡å®ä½“å", "type": "å…³ç³»ç±»å‹", "properties": {"source_sentence": "æ”¯æŒè¯¥å…³ç³»çš„åŸæ–‡å¥å­"}},
    ...
  ]
}

é‡è¦ï¼šæ¯ä¸ªå…³ç³»å¿…é¡»åŒ…å«source_sentenceå±æ€§ï¼Œç”¨äºåç»­æ¨ç†åˆ†æã€‚
ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¿”å›JSONã€‚"""

    return autogen.AssistantAgent(
        name="EntityExtractor", 
        system_message=system_message,
        llm_config=llm_config,
        max_consecutive_auto_reply=1,
    )


def register_reasoning_functions(agent: autogen.AssistantAgent, user_proxy: autogen.UserProxyAgent, graph_db: GraphDB):
    """
    ä¸ºæ™ºèƒ½ä½“æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°
    
    Args:
        agent: è¦æ³¨å†Œå‡½æ•°çš„æ™ºèƒ½ä½“
        user_proxy: ç”¨æˆ·ä»£ç†
        graph_db: å›¾æ•°æ®åº“å®ä¾‹
    """
    
    # åˆ›å»ºåŒ…è£…å‡½æ•°ï¼Œä»¥ä¾¿ä¼ é€’graph_dbå‚æ•°
    def find_patterns_wrapper():
        """å‘ç°å›¾è°±ä¸­çš„æœ‰è¶£æ¨¡å¼"""
        return find_interesting_patterns(graph_db)
    
    def verify_hypothesis_wrapper(pattern: Dict[str, Any]):
        """éªŒè¯æ¨¡å¼å‡è®¾"""
        return verify_hypothesis_from_text(pattern, graph_db)
    
    def create_relationship_wrapper(verified_pattern: Dict[str, Any]):
        """åˆ›å»ºæ¨ç†å…³ç³»"""
        return create_inferred_relationship(verified_pattern, graph_db)
    
    # æ³¨å†Œå‡½æ•°åˆ°æ™ºèƒ½ä½“
    autogen.register_function(
        find_patterns_wrapper,
        caller=agent,
        executor=user_proxy,
        name="find_interesting_patterns",
        description="åœ¨çŸ¥è¯†å›¾è°±ä¸­å‘ç°æ½œåœ¨çš„æœ‰è¶£æ¨¡å¼ï¼Œç”¨äºæ¨ç†åˆ†æ"
    )
    
    autogen.register_function(
        verify_hypothesis_wrapper,
        caller=agent,
        executor=user_proxy,
        name="verify_hypothesis_from_text",
        description="éªŒè¯å‘ç°çš„æ¨¡å¼æ˜¯å¦æ„æˆæœ‰æ„ä¹‰çš„éšå«å…³ç³»"
    )
    
    autogen.register_function(
        create_relationship_wrapper,
        caller=agent,
        executor=user_proxy,
        name="create_inferred_relationship",
        description="å°†éªŒè¯ä¸ºé«˜ç½®ä¿¡åº¦çš„éšå«å…³ç³»å†™å…¥å›¾è°±"
    )


def extract_json_from_response(response_content: str) -> Optional[Dict]:
    """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
    try:
        # ç›´æ¥å°è¯•è§£æ
        return json.loads(response_content.strip())
    except json.JSONDecodeError:
        try:
            # å°è¯•æå–ä»£ç å—ä¸­çš„JSON
            if '```json' in response_content:
                start_marker = '```json'
                end_marker = '```'
                start_idx = response_content.find(start_marker) + len(start_marker)
                end_idx = response_content.find(end_marker, start_idx)
                if end_idx != -1:
                    json_content = response_content[start_idx:end_idx].strip()
                    return json.loads(json_content)
            elif '```' in response_content:
                parts = response_content.split('```')
                if len(parts) >= 3:
                    json_content = parts[1].strip()
                    return json.loads(json_content)
            
            # å°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
            start_idx = response_content.find('{')
            end_idx = response_content.rfind('}') + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = response_content[start_idx:end_idx]
                return json.loads(json_str)
                
        except json.JSONDecodeError:
            pass
    
    return None


def save_to_graph_db(ontology: Dict, entities_and_relations: Dict, graph_db: GraphDB) -> bool:
    """å°†æå–çš„æ•°æ®ä¿å­˜åˆ°å›¾æ•°æ®åº“"""
    try:
        logger.info("å¼€å§‹ä¿å­˜æ•°æ®åˆ°å›¾æ•°æ®åº“...")
        
        # å¯¼å…¥å®ä½“
        entities = entities_and_relations.get('entities', [])
        logger.info(f"å‡†å¤‡å¯¼å…¥ {len(entities)} ä¸ªå®ä½“")
        
        for entity in entities:
            success = graph_db.import_entity(entity)
            if success:
                logger.info(f"æˆåŠŸå¯¼å…¥å®ä½“: {entity['name']}")
            else:
                logger.warning(f"å¯¼å…¥å®ä½“å¤±è´¥: {entity['name']}")
        
        # å¯¼å…¥å…³ç³»
        relationships = entities_and_relations.get('relationships', [])
        logger.info(f"å‡†å¤‡å¯¼å…¥ {len(relationships)} ä¸ªå…³ç³»")
        
        for relationship in relationships:
            success = graph_db.import_relationship(relationship)
            if success:
                logger.info(f"æˆåŠŸå¯¼å…¥å…³ç³»: {relationship['source']} -> {relationship['target']}")
            else:
                logger.warning(f"å¯¼å…¥å…³ç³»å¤±è´¥: {relationship['source']} -> {relationship['target']}")
        
        logger.info("æ•°æ®å¯¼å…¥å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ°å›¾æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        return False


def main():
    """ä¸»å‡½æ•° - å®Œæ•´çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆå’Œæ¨ç†æµç¨‹"""
    logger.info("ğŸš€ AutoGençŸ¥è¯†å›¾è°±ç”Ÿæˆç³»ç»Ÿå¯åŠ¨ (å«é«˜çº§æ¨ç†åŠŸèƒ½)")
    
    try:
        # 1. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        logger.info("ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        graph_db = GraphDB(
            uri=config.NEO4J_URI,
            username=config.NEO4J_USERNAME,
            password=config.NEO4J_PASSWORD
        )
        
        if not graph_db.connected:
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # 2. è¯»å–æµ‹è¯•æ–‡æœ¬
        logger.info("ğŸ“– è¯»å–æµ‹è¯•æ–‡æœ¬...")
        text_file = Path("data/sample_text.txt")
        
        if not text_file.exists():
            logger.error(f"âŒ æ–‡æœ¬æ–‡ä»¶æœªæ‰¾åˆ°: {text_file}")
            return
        
        with open(text_file, 'r', encoding='utf-8') as f:
            sample_text = f.read().strip()
        
        logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡æœ¬ ({len(sample_text)} å­—ç¬¦)")
        
        # 3. åˆ›å»ºåŸºç¡€æ™ºèƒ½ä½“
        logger.info("ğŸ¤– åˆ›å»ºåŸºç¡€æ™ºèƒ½ä½“...")
        
        llm_config = config.llm_config_gpt4
        
        ontologist = create_simplified_ontologist_agent(llm_config)
        extractor = create_entity_extraction_agent(llm_config)
        
        user_proxy = autogen.UserProxyAgent(
            name="UserProxy",
            code_execution_config=False,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=0,
        )
        
        logger.info("âœ… åŸºç¡€æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")
        
        # 4. ç¬¬ä¸€é˜¶æ®µï¼šæœ¬ä½“è®¾è®¡
        logger.info("\nğŸ§  ç¬¬ä¸€é˜¶æ®µï¼šæœ¬ä½“è®¾è®¡")
        logger.info("=" * 50)
        
        ontology_task = f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬è®¾è®¡çŸ¥è¯†å›¾è°±æ¶æ„ï¼š\n\n{sample_text}"
        
        user_proxy.initiate_chat(ontologist, message=ontology_task, max_turns=1)
        ontology_response = user_proxy.last_message(agent=ontologist)["content"]
        
        logger.info(f"æœ¬ä½“è®ºä¸“å®¶å“åº”: {ontology_response}")
        
        ontology_data = extract_json_from_response(ontology_response)
        if not ontology_data:
            logger.error("âŒ æ— æ³•è§£ææœ¬ä½“è®¾è®¡å“åº”")
            return
        
        logger.info(f"âœ… æœ¬ä½“è®¾è®¡å®Œæˆ: {len(ontology_data.get('node_labels', []))} ä¸ªå®ä½“ç±»å‹, {len(ontology_data.get('relationship_types', []))} ä¸ªå…³ç³»ç±»å‹")
        
        # 5. ç¬¬äºŒé˜¶æ®µï¼šå®ä½“å’Œå…³ç³»æå–  
        logger.info("\nğŸ” ç¬¬äºŒé˜¶æ®µï¼šå®ä½“å’Œå…³ç³»æå–")
        logger.info("=" * 50)
        
        extraction_task = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œéµå¾ªè®¾è®¡çš„æœ¬ä½“æ¶æ„ï¼š

æœ¬ä½“æ¶æ„ï¼š
- å®ä½“ç±»å‹: {', '.join(ontology_data.get('node_labels', []))}
- å…³ç³»ç±»å‹: {', '.join(ontology_data.get('relationship_types', []))}

æ–‡æœ¬å†…å®¹ï¼š
{sample_text}"""
        
        user_proxy.initiate_chat(extractor, message=extraction_task, max_turns=1)
        extraction_response = user_proxy.last_message(agent=extractor)["content"]
        
        logger.info(f"å®ä½“æå–ä¸“å®¶å“åº”: {extraction_response}")
        
        entities_data = extract_json_from_response(extraction_response)
        if not entities_data:
            logger.error("âŒ æ— æ³•è§£æå®ä½“æå–å“åº”")
            return
        
        entities_count = len(entities_data.get('entities', []))
        relations_count = len(entities_data.get('relationships', []))
        logger.info(f"âœ… å®ä½“æå–å®Œæˆ: {entities_count} ä¸ªå®ä½“, {relations_count} ä¸ªå…³ç³»")
        
        # 6. ç¬¬ä¸‰é˜¶æ®µï¼šä¿å­˜åˆ°å›¾æ•°æ®åº“
        logger.info("\nğŸ’¾ ç¬¬ä¸‰é˜¶æ®µï¼šä¿å­˜åˆ°å›¾æ•°æ®åº“")
        logger.info("=" * 50)
        
        success = save_to_graph_db(ontology_data, entities_data, graph_db)
        if not success:
            logger.error("âŒ ä¿å­˜åˆ°å›¾æ•°æ®åº“å¤±è´¥")
            return
        
        logger.info("âœ… åŸºç¡€å›¾è°±æ„å»ºå®Œæˆ")
        
        # 7. ç¬¬å››é˜¶æ®µï¼šé«˜çº§æ¨ç†åˆ†æ
        logger.info("\nğŸ§  ç¬¬å››é˜¶æ®µï¼šé«˜çº§æ¨ç†åˆ†æ")
        logger.info("=" * 60)
        
        # åˆ›å»ºæ¨ç†æ™ºèƒ½ä½“
        reasoning_agent = create_advanced_reasoning_agent(llm_config)
        reasoning_proxy = create_reasoning_user_proxy()
        
        # æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°
        logger.info("ğŸ”§ æ³¨å†Œæ¨ç†å·¥å…·å‡½æ•°...")
        register_reasoning_functions(reasoning_agent, reasoning_proxy, graph_db)
        logger.info("âœ… æ¨ç†å·¥å…·å‡½æ•°æ³¨å†Œå®Œæˆ")
        
        # å¯åŠ¨æ¨ç†åˆ†æ
        logger.info("ğŸ” å¯åŠ¨æ·±åº¦æ¨ç†åˆ†æ...")
        
        reasoning_task = "è¯·å¼€å§‹å¯¹ç°æœ‰å›¾è°±è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æï¼Œå‘ç°å¹¶éªŒè¯éšå«çš„çŸ¥è¯†å…³ç³»ã€‚"
        
        # å¼€å§‹æ¨ç†å¯¹è¯
        reasoning_proxy.initiate_chat(
            reasoning_agent, 
            message=reasoning_task,
            max_turns=20  # å…è®¸è¶³å¤Ÿçš„è½®æ¬¡å®Œæˆæ¨ç†
        )
        
        # 8. æŸ¥çœ‹æœ€ç»ˆç»“æœ
        logger.info("\nğŸ“Š ç¬¬äº”é˜¶æ®µï¼šç»“æœéªŒè¯")
        logger.info("=" * 50)
        
        # æŸ¥è¯¢æ¨ç†å…³ç³»
        inferred_query = """
        MATCH (a)-[r]->(b) 
        WHERE r.type = 'INFERRED'
        RETURN a.name as source, type(r) as rel_type, b.name as target, r.confidence as confidence
        ORDER BY r.confidence DESC
        """
        
        inferred_relations = graph_db.execute_query(inferred_query)
        
        if inferred_relations:
            logger.info(f"ğŸ‰ æˆåŠŸåˆ›å»ºäº† {len(inferred_relations)} ä¸ªæ¨ç†å…³ç³»:")
            for rel in inferred_relations:
                logger.info(f"   {rel['source']} --[{rel['rel_type']}]--> {rel['target']} (ç½®ä¿¡åº¦: {rel['confidence']})")
        else:
            logger.info("âš ï¸  æœªå‘ç°æ–°çš„æ¨ç†å…³ç³»")
        
        # æŸ¥è¯¢æ€»ä½“å›¾è°±ç»Ÿè®¡
        stats_query = """
        MATCH (n) 
        WITH labels(n) as node_labels
        UNWIND node_labels as label
        RETURN label, count(*) as count
        ORDER BY count DESC
        """
        
        node_stats = graph_db.execute_query(stats_query)
        
        logger.info("\nğŸ“ˆ æœ€ç»ˆå›¾è°±ç»Ÿè®¡:")
        total_nodes = 0
        for stat in node_stats:
            count = stat['count']
            total_nodes += count
            logger.info(f"   {stat['label']}: {count} ä¸ªèŠ‚ç‚¹")
        
        rel_stats_query = """
        MATCH ()-[r]->() 
        RETURN type(r) as rel_type, count(r) as count
        ORDER BY count DESC
        """
        
        rel_stats = graph_db.execute_query(rel_stats_query)
        
        total_rels = 0
        for stat in rel_stats:
            count = stat['count']
            total_rels += count
            logger.info(f"   {stat['rel_type']}: {count} ä¸ªå…³ç³»")
        
        logger.info(f"\nğŸ¯ ç³»ç»Ÿå®Œæˆï¼æ€»è®¡: {total_nodes} ä¸ªèŠ‚ç‚¹, {total_rels} ä¸ªå…³ç³»")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if 'graph_db' in locals() and hasattr(graph_db, 'close'):
            graph_db.close()


if __name__ == "__main__":
    main() 